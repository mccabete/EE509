---
title: "Tess_McCabe_Lab5"
author: "Tess McCabe"
date: "2/20/2018"
output:
  html_document: default
  pdf_document: default
---


## A simple JAGS model
As our first example, let’s consider the simple case of finding the mean of a Normal distribution with a known variance and Normal prior on the mean:

$$prior = P(\mu) = N(\mu | \mu_0 , \tau)$$ $$L = P(X | \mu) = N( X | \mu, \sigma^2)$$

As noted earlier JAGS parameterizes the Normal distribution in terms of a mean and precision (1/variance), rather than a mean and standard deviation. Therefore, let's redefine the prior precision as $T = 1/\tau$ and the data precision as $S = 1/\sigma^2$. Let's also switch from the conditional notation for PDFs to the tilda notation (~ is read as "is distributed as"). Given this we can rewrite the above model as:

$$ \mu \sim N(\mu_0,T) $$ $$ X \sim N(\mu, S) $$

This problem has an exact analytical solution so we can compare the numerical results to the analytical one (Dietze 2016, Eqn 5.7).

$$ P(\mu | X) \sim N \left( {{S}\over{S+T}}X + {{T}\over{S+T}}\mu_0,S+T\right) $$

In JAGS the specification of any model begins with the word “model” and then encapsulates the rest of the model code in curly brackets:
```{r, include=TRUE, echo=TRUE}
library(rjags)

NormalMean = "
model {
mu ~ dnorm(mu0,T) # prior on the mean 
X ~ dnorm(mu,S) # data model
}
"

sigma = 5  ## prior standard deviation
data = list(X = 42, ## data
            mu0 = 53, ## prior mean
            S = 1/sigma^2, ## prior precision
            T = 1/185) ## data precision

inits = list(mu=50)

inits <- list()
inits[[1]] <- list(mu = 40)
inits[[2]] <- list(mu = 45)
inits[[3]] <- list(mu = 50)

j.model   <- jags.model (file = textConnection(NormalMean),
                             data = data,
                             inits = inits,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("mu"),
                                n.iter = 1000)

```
 # Evaluating MCMC outputs
 
```{r, include=TRUE, echo=TRUE}
library(rjags)
library(coda)

plot(jags.out)
gelman.diag(jags.out) # So really really convereged

GBR <- gelman.plot(jags.out)

## determine the first iteration after convergence
burnin <- GBR$last.iter[tail(which(GBR$shrink[,,2] > 1.1),1)+1]
## check for no burn-in case
if(length(burnin) == 0) burnin = 1
## remove burn-in
jags.burn <- window(jags.out,start=burnin)
## check diagnostics post burn-in
gelman.diag(jags.burn)
plot(jags.burn)


# Autocorelation Plot
acfplot(jags.burn)
effectiveSize(jags.burn) # Pretty good for these parameters

cumuplot(jags.out,probs=c(0.025,0.25,0.5,0.75,0.975))
```


```{r, include=TRUE, echo=TRUE}
summary(jags.out) #Why not jags.burn
out <- as.matrix(jags.out)
```

## Case Study: Forest Stand Charactoristics

```{r, echo=FALSE}
data = list(N = 297, mu0=20, S =1/27, T=0.01, X = c(20.9, 13.6, 15.7, 6.3, 2.7, 25.6, 4, 20.9, 7.8, 27.1, 25.2, 19, 17.8, 22.8, 12.5, 21.1, 22, 22.4, 5.1, 16, 20.7, 15.7, 5.5, 18.9, 22.9, 15.5, 18.6, 19.3, 14.2, 12.3, 11.8, 26.8, 17, 5.7, 12, 19.8, 19, 23.6, 19.9, 8.4, 22, 18.1, 21.6, 17, 12.4, 2.9, 22.6, 20.8, 18.2, 14.2, 17.3, 14.5, 8.6, 9.1, 2.6, 19.8, 20, 22.2, 10.2, 12.9, 20.9, 21.1, 7.3, 5.8, 23.1, 17, 21.5, 10.1, 18.4, 22.6, 21.2, 21.5, 22.4, 17.3, 16, 25, 22.4, 23.9, 23, 21.9, 19, 28.6, 16, 22.5, 23.2, 8.7, 23.4, 15.3, 25.6, 19.2, 17.4, 23.8, 20.4, 19, 3.6, 23.4, 19.6, 17.5, 16.5, 22, 19.7, 7.35, 18, 17.8, 9.6, 15, 12, 17.7, 21.4, 17, 22.1, 18.9, 15.05, 12.9, 19.3, 15.3, 13.6, 15.4, 10.6, 11.3, 11.8, 22.2, 22.2, 13.1, 7.4, 4.5, 11.7, 19.5, 19.9, 11.6, 13.9, 15.5, 11, 18.6, 17.6, 12.7, 20.9, 18.8, 22.4, 21.2, 18.2, 15.3, 13.6, 7.3, 17.4, 17.4, 10.5, 22.9, 23.2, 13.8, 14.8, 22.2, 20.9, 13, 18.9, 19, 15.2, 16.8, 18, 24.6, 15.4, 17.2, 23.2, 22.8, 25.5, 7.8, 6, 6.4, 19, 13.5, 23.7, 18, 22.2, 22.4, 9.3, 13.7, 18.9, 20.5, 23.3, 20.8, 18.4, 4.5, 12.2, 16.9, 13.5, 17.8, 16.9, 20.4, 19.5, 22.2, 24.5, 21.2, 16.5, 18, 16.4, 3.9, 17.9, 22, 12.9, 21, 18, 9.2, 15.9, 8.1, 8.3, 10.7, 12, 19.9, 13.6, 17.3, 11.5, 12.4, 15.1, 22, 19.3, 17.5, 14.5, 14.7, 17.5, 19.6, 12.9, 20.3, 17.9, 20.2, 18.3, 9.5, 19, 21, 13.1, 20.4, 16.3, 18.3, 11.8, 23.3, 15.2, 20, 17.9, 12, 19.6, 18.5, 16.2, 10.9, 17.8, 13.8, 10, 17.9, 15.6, 20.3, 14.9, 18.6, 12.5, 18.2, 16, 18.7, 18, 15.3, 19, 17.9, 15.8, 17.7, 14.4, 19.6, 18.3, 18.7, 17.8, 18, 10.1, 18.8, 16.4, 21.2, 16.6, 16.7, 17.8, 16.5, 19.3, 16.3, 14.2, 13, 9.4, 19.7, 13.4, 2.6, 17.6, 16.7, 17.6, 5.8, 17.6, 20.1, 18.2, 16.7, 14, 13.9, 5.1, 16.6, 3.9, 17.5, 18))

```

```{r, include=TRUE, echo=TRUE}


NormalMeanN <- "
model {
  mu ~ dnorm(mu0,T) # prior on the mean 
  for(i in 1:N){
    X[i] ~ dnorm(mu,S) # data model
  }
}
"

inits_tess<-list()
inits_tess[[1]] <- list(mu = mean(data$X))
inits_tess[[2]] <- list(mu = (mean(data$X)-(mean(data$X)*(1/4))))
inits_tess[[3]] <- list(mu = mean(data$X)+(mean(data$X)*(1/4)))

j.model   <- jags.model (file = textConnection(NormalMeanN),
                             data = data,
                             inits = inits_tess,
                             n.chains = 3)
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("mu"),
                                n.iter = 1000)
#plot(jags.out) #Not realy yet, haven't subset the burn in. Just checking it out. 
#summary(jags.out)

GBR <- gelman.plot(jags.out)
GBR

## determine the first iteration after convergence
burnin <- GBR$last.iter[tail(which(GBR$shrink[,,2] > 1.01),1)+1] # Changed the Burn-in window, becuase when I looked at a cumplot(jags.burn) it looked pretty similar to cumplot(jags.out)

if(length(burnin) == 0) burnin = 1
jags.burn <- window(jags.out,start=burnin)
gelman.diag(jags.burn) # I'd say that's pretty close to one
plot(jags.burn)

effectiveSize(jags.burn) # effective sample size >2000 so pretty good for a mean estimate


cumuplot(jags.burn,probs=c(0.025,0.25,0.5,0.75,0.975)) # seems to have stable quantiles 

plot(jags.out)
summary(jags.burn)

```
Justification: 
The settings I used were:  
- three chains
- a mean*1.25
- a mean*0.75
- a smaller burn-in window

I used three chains because I'd seen 2 chains behave appropriately in the example you provided, and I wanted to be able to use the Brooks-Gelman-Rubin statistic, which requires multiple chains. I choose my initial parameter guesses based on the data. I figured the mean would be a pretty good guess, and that some bounds around the mean would good practice. I choose a smaller burn-in window because I saw that the $gelman.diag(jags.burn)$ function returned 1, which made me think that the MCMC's "burn-in" period was probably small, and that I could afford to be a little more discriminating. I ran the effective sample size twice (once before and one after changing the burn-in window) my effective sample size was always higher than 2000. 

```{r, echo=FALSE}
data = list(N = 297, mu0=20, T=0.01, x=1:297, R = 1, X = c(20.9, 13.6, 15.7, 6.3, 2.7, 25.6, 4, 20.9, 7.8, 27.1, 25.2, 19, 17.8, 22.8, 12.5, 21.1, 22, 22.4, 5.1, 16, 20.7, 15.7, 5.5, 18.9, 22.9, 15.5, 18.6, 19.3, 14.2, 12.3, 11.8, 26.8, 17, 5.7, 12, 19.8, 19, 23.6, 19.9, 8.4, 22, 18.1, 21.6, 17, 12.4, 2.9, 22.6, 20.8, 18.2, 14.2, 17.3, 14.5, 8.6, 9.1, 2.6, 19.8, 20, 22.2, 10.2, 12.9, 20.9, 21.1, 7.3, 5.8, 23.1, 17, 21.5, 10.1, 18.4, 22.6, 21.2, 21.5, 22.4, 17.3, 16, 25, 22.4, 23.9, 23, 21.9, 19, 28.6, 16, 22.5, 23.2, 8.7, 23.4, 15.3, 25.6, 19.2, 17.4, 23.8, 20.4, 19, 3.6, 23.4, 19.6, 17.5, 16.5, 22, 19.7, 7.35, 18, 17.8, 9.6, 15, 12, 17.7, 21.4, 17, 22.1, 18.9, 15.05, 12.9, 19.3, 15.3, 13.6, 15.4, 10.6, 11.3, 11.8, 22.2, 22.2, 13.1, 7.4, 4.5, 11.7, 19.5, 19.9, 11.6, 13.9, 15.5, 11, 18.6, 17.6, 12.7, 20.9, 18.8, 22.4, 21.2, 18.2, 15.3, 13.6, 7.3, 17.4, 17.4, 10.5, 22.9, 23.2, 13.8, 14.8, 22.2, 20.9, 13, 18.9, 19, 15.2, 16.8, 18, 24.6, 15.4, 17.2, 23.2, 22.8, 25.5, 7.8, 6, 6.4, 19, 13.5, 23.7, 18, 22.2, 22.4, 9.3, 13.7, 18.9, 20.5, 23.3, 20.8, 18.4, 4.5, 12.2, 16.9, 13.5, 17.8, 16.9, 20.4, 19.5, 22.2, 24.5, 21.2, 16.5, 18, 16.4, 3.9, 17.9, 22, 12.9, 21, 18, 9.2, 15.9, 8.1, 8.3, 10.7, 12, 19.9, 13.6, 17.3, 11.5, 12.4, 15.1, 22, 19.3, 17.5, 14.5, 14.7, 17.5, 19.6, 12.9, 20.3, 17.9, 20.2, 18.3, 9.5, 19, 21, 13.1, 20.4, 16.3, 18.3, 11.8, 23.3, 15.2, 20, 17.9, 12, 19.6, 18.5, 16.2, 10.9, 17.8, 13.8, 10, 17.9, 15.6, 20.3, 14.9, 18.6, 12.5, 18.2, 16, 18.7, 18, 15.3, 19, 17.9, 15.8, 17.7, 14.4, 19.6, 18.3, 18.7, 17.8, 18, 10.1, 18.8, 16.4, 21.2, 16.6, 16.7, 17.8, 16.5, 19.3, 16.3, 14.2, 13, 9.4, 19.7, 13.4, 2.6, 17.6, 16.7, 17.6, 5.8, 17.6, 20.1, 18.2, 16.7, 14, 13.9, 5.1, 16.6, 3.9, 17.5, 18))

```

```{r, echo= TRUE}

library(rjags)
library(coda)

NormalMeanN <- "
model {
  S ~ dexp(R) # Prior on varience
  mu ~ dnorm(mu0,T) # prior on the mean 
  for(i in 1:N){
    X[i] ~ dnorm(mu,S) # data model
  }
}
"

inits_tess<-list()
inits_tess[[1]] <- list(mu = mean(data$X), S= 0.5)
inits_tess[[2]] <- list(mu = (mean(data$X)-(mean(data$X)*(1/4))), S = 2)
inits_tess[[3]] <- list(mu = mean(data$X)+(mean(data$X)*(1/4)), S = 1)

j.model   <- jags.model (file = textConnection(NormalMeanN),
                             data = data,
                             inits = inits_tess,
                             n.chains = 3)
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("mu", "S"),
                                n.iter = 1000)
#plot(jags.out) #Not realy yet, haven't subset the burn in. Just checking it out. 
#summary(jags.out)

GBR <- gelman.plot(jags.out)
GBR ## looks like post 300 both have convereged



jags.burn <- window(jags.out, start= 300, end=1000)
gelman.diag(jags.burn) # I'd say that's pretty close to one
plot(jags.burn)

effectiveSize(jags.burn) # effective sample size >2000 so pretty good for a mean estimate


cumuplot(jags.burn,probs=c(0.025,0.25,0.5,0.75,0.975)) # seems to have stable quantiles 


summary(jags.burn)


```
Justification: 
The settings I used were:  
- an exponential prior on the variance

I used an exponential prior on the variance because I knew I wanted it to be zero bounded, but with no cap. I thought it probably shouldn't be skewed towards zero. My initial choices for S were chose randomly, as I don't really have a good seance for how variance behaves. 
