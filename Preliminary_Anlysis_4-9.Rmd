---
title: "Preliminary Analysis 4/9"
author: "Tess McCabe"
date: "4/4/2018"
output: html_document
---

```{r, include= FALSE}
library(coda)
library(rjags)
library(dplyr)
require(devtools)
```

## Data reference

My data is called ```final```. The four variables I use are ```final$time_since_last_burn```, ```final$litter_mass_wet```, ```final$collection_method``` and ```final$count```. ```final$time_since_last_burn``` is the number of years since the plot was burned. ```final$litter_mass_wet``` is the litter biomass collected at the plot. ```final$count``` is counts of ticks per plot and ```final$collection_method``` is how they were collected. 

```{r, include= FALSE}
## read in data

ticks<-read.csv("~/Documents/Work/SERDP/SERDP/ticks.csv", header = TRUE)
ticks$plot_id<-tolower(ticks$plot_id)
#cogongrass<-read.csv("~/Dropbox/SERDP-Project/data/cogon-data.csv", header = TRUE)# get cogongrass biomass
fuels<-read.csv("~/Dropbox/SERDP-Project/data/quadrat25cm.csv", header = TRUE) # get litter biomass, need to remove cogongrass as redundant
fire<-read.csv("~/Dropbox/SERDP-Project/data/plot-info.csv", header = TRUE) #Get year last burnt
fire$burn_unit_id<-tolower(fire$burn_unit_id)
fire$burn_unit_id


for(i in 1:length(fuels$plot_id)){
  
  if(substr(fuels$plot_id[i],1,1) %in% fire$burn_unit_id){
    
    matching_id<- fire[fire$burn_unit_id==substr(fuels$plot_id[i],1,1),]
    matching_id<-matching_id[matching_id$instal == fuels$installation[i],]
  
    
    year_burn<-unique(matching_id$fire_year)
    
    
  fuels$time_since_last_burn[i]<-2018-year_burn
    
  }
    
}


final<-left_join(fuels, ticks, by= "plot_id")
```
## How I Chose My Priors

Over the course of my model analysis, I chose several priors. Some of which were slightly informative, some of which were intentionally not. The three terms that need priors are: 

  - phi ~ dunif(min,max) 

Phi is meant to represent the minimum amount of vegetation every tick needs to survive. This was a difficult parameter to research in the literature, in part because why a tick would "need" vegetation depends on context (breeding, shelter, access to hosts). My chosen vegetation metric (litter biomass) might lean towards a shelter/ breeding interpretation, but in any case, I tried to leave this as an uninformative prior between 0.001 and 10. More research could constrain the order of magnitude a bit. 

  - r ~  dunif(min1, max1)	

 r represents the population growth of ticks after a fire. Knowing nothing about tick population, I made the prior span a few orders of magnitude. This could also likely be constrained with a dive into tick population literature. In looking at some tick sampling examples, this prior could be too confidant - I saw one source that listed tens of thousands of larva (However, because our sampling didn't include larva, my estimate might be interpreted as reproducing and living until at least nymph stage).

  - N_0 ~ dunif(prob, size)

The N_0 parameter is meant to represent the instantaneous population immediately post fire.I wanted something that represented how it was possible that the population would be pretty devastated by fire. 
  - mu ~ dnorm(lambda_prior, st)
  
mu is modeled as a normal distribution that represents what I think the true population density looks like.  I chose a normal because I thought that while negative numbers would be wrong, I knew that I would be setting the mean prior to a fairly large number ( ~ 77) , so that would likely not be an issue. My mean comes from the mean population value a got from a [paper](https://doaj.org/article/21e4a932ac87438ebb5f1804dc455a20) that sample ticks in Georgia, also in frequently burned forests. I left the precision small (1), because I had no idea how that number would translate to Florida, or if the sampling  methods of the above paper was more or less effective at sampling the population. 


## How I Got My "Known" Values

$$area_{whalen}= 1/500\ m^2$$ 
This is the area of the plot that the field ecologist (Whalen) was walking around in when he found the ticks. It's written as a fraction so so that fitting in jags is easier.

$$area_{trap} = 1/5 m^2$$
The area that the trick traps draw from. 

$$time_{whalen}= 3.5 \ hours$$ 

The average number of hours Whalen spent on each plot. It might be possible to model this, either as a distribution between 2- 5 hours, or as a function of plot qualities, like the number of trees in each plot. According to Whalen, the largest thing that slows him down is the number of trees in the plot. The reason I didn't model is was I ran into separability problems with this term, and the efficiency term. 

$$time_{trap} = 24 \ hours$$
Time the traps were left out to collect ticks. 

$$efficientcy = 0.0001$$

This represents how effective Whalen is sampling ticks relative to the tick traps.  


## Tick population mean model 


```{r, include= TRUE, echo=TRUE, warning=FALSE}

x_whalen<-na.omit(final$count[final$collection_method== "Whalen" |final$collection_method== "Drag" ])
x_trap<-na.omit(final$count[final$collection_method== "trap" |final$collection_method== "people" ])

n_whalen<-length(na.omit(final$count[final$collection_method== "Whalen" |final$collection_method== "Drag" ]))
n_trap<-length(na.omit(final$count[final$collection_method== "trap" |final$collection_method== "people" ]))

data <- list(x_whalen = x_whalen, x_trap= x_trap, n_trap = n_trap, n_whalen= n_whalen)


mean_model = "
model {
mu ~ dnorm(lambda_prior, 1) # prior on the mean 
  for(i in 1:n_whalen){
	   #x_whalen[i]	## process model
    x_whalen[i] ~ dpois((mu*efficiency*time_whalen)*area_whalen)
  }
for(i in 1:n_trap){
    #x_trap[i]     ## process model
    x_trap[i] ~ dpois(mu*area_trap*time_trap)
  }

}"




## specify priors
data$lambda_prior<- 76.89473684  #density of tick in m^2
#data$a <- 2                ## minimum hours spent on plot
#data$b <- 5                   ## maximum hours spend on plot
data$efficiency<-0.0001 #otherwise not seperable

## Specify known values
data$area_whalen <- 1/500 # meters squared
data$area_trap <- 1/5 # meters squared
data$time_whalen<-3.5 # This is an average of a varaible unit (2-5 hours /plot). Could model this, but would then wouldn't be seperatble from efficietcy or mu
data$time_trap<-24 # hours

## initial conditions
nchain = 3
inits <- list()
for(i in 1:nchain){
 inits[[i]] <- list( mu = 50)
}

j.model.mean   <- rjags::jags.model(file = textConnection(mean_model),
                             data = data,
                             inits = inits,
                             n.chains = nchain)

## export R envoronment to geo for model run



var.out.mean   <-rjags::coda.samples (model = j.model.mean,
                            variable.names = c( "mu"),
                                n.iter = 2000)

```


**Checking for convergence**
```{r, include = FALSE}

#coda::gelman.plot(var.out.mean)

var.burn.mean<-window(var.out.mean, 2001)
var.mat.mean<-as.matrix(var.burn.mean)

print(coda::effectiveSize(var.burn.mean)) # pleanty

print(summary(var.burn.mean))
```

** Results **
```{r, echo=TRUE}
hist(na.omit(final$count), main  = "")
title("Ticks Counted" )

hist(var.mat.mean, main  = "")
title("Modeled Tick population distribution (per m^2)" )
```


## Linear regression model

```{r, echo=TRUE, warning=FALSE}

x_whalen<-na.omit(final$count[final$collection_method== "Whalen" | final$collection_method== "Drag" ])
x_trap<-na.omit(final$count[final$collection_method== "trap" | final$collection_method== "people" ])

x_whalen<-x_whalen[-c(9,10)] # edited due to missing data

veg_whalen<-na.omit(final$litter_mass_wet[final$collection_method== "Whalen" |final$collection_method == "Drag" ])
veg_trap<-na.omit(final$litter_mass_wet[final$collection_method== "trap" | final$collection_method== "people" ])

n_whalen<-length(x_whalen)
n_trap<-length(x_trap)

data <- list(x_whalen = x_whalen, x_trap= x_trap, n_trap = c(n_trap, length(veg_trap)), n_whalen = c(n_whalen, length(veg_whalen)), veg_whalen = veg_whalen, veg_trap = veg_trap)


univariate_regression = "
model {
phi ~ dunif(min,max)  	## prior on metabolic rate
  

  for(i in 1:n_whalen[1]){
	   mu[i]<-veg_whalen[i]*phi	## process model
    x_whalen[i] ~ dpois((mu[i]*efficiency*time_whalen)*area_whalen)
  }
for(i in 1:n_trap[1]){
    mu[n_whalen[1]+i]<-veg_trap[i]*phi
    x_trap[i] ~ dpois(mu[i]*area_trap*time_trap)
  }

}"


## Specify priors
data$min <- 0.0001      
data$max <-10  

## Specify "known" values 
data$efficiency<-0.0001  # otherwise not seperable
data$area_whalen <- 1/500 # meters squared
data$area_trap <- 1/5 # meters squared
data$time_whalen<-3.5 # This is an average of a varaible unit (2-5 hours /plot). Could model this, but would then wouldn't be seperatble from efficietcy or mu
data$time_trap<-24 # hours


## initial conditions
nchain = 3
inits <- list()
for(i in 1:nchain){
 inits[[i]] <- list(efficietcy = runif(1, 0.001, 1/2))
}

j.model.regression   <- rjags::jags.model(file = textConnection(univariate_regression),
                             data = data,
                             inits = inits,
                             n.chains = nchain)


var.out.regression   <-rjags::coda.samples(model = j.model.regression, variable.names = c("phi"), n.iter = 2000)

```

**Checking for convergence**
```{r, include = FALSE, warning=FALSE, echo=TRUE}

#coda::gelman.plot(var.out.regression)

var.burn.regression<-window(var.out.regression, 1500)
var.mat.regression<-as.matrix(var.burn.regression)

print(coda::effectiveSize(var.burn.regression)) #pleanty 
print(summary(var.burn.regression))
```
**Results **

```{r, echo= TRUE, warning=FALSE}

## credible and prediction intervals
nsamp <- 300
samp <- sample.int(nrow(var.mat.regression),nsamp)
xpred <- 0:200  					## sequence of x values we're going to
npred <- length(xpred)				##      make predictions for
ypred <- matrix(0.0,nrow=nsamp,ncol=npred)	## storage for predictive interval
ycred <- matrix(0.0,nrow=nsamp,ncol=npred)	## storage for credible interval



for(g in seq_len(nsamp)){
  phi = var.mat.regression[samp[g],]
  ycred[g,] <- phi*xpred*c(veg_whalen, veg_trap)[g]
  ypred[g,] <- rnorm(npred,ycred[g,]*data$area_trap*data$time_trap,ycred[g,]*data$area_trap*data$time_trap)
}

ci <- apply(ycred,2,quantile,c(0.025,0.5,0.975), na.rm = TRUE)  ## credible interval and median
pi <- apply(ypred,2,quantile,c(0.025,0.975), na.rm= TRUE)		## prediction interval

plot(final$litter_mass_wet, final$count, xlab= "Litter Biomass", ylab = "Tick Counts", ylim = c(0, 200))
lines(xpred,ci[1,],col=3,lty=2)	## lower CI
lines(xpred,ci[2,],col=3,lwd=3)	## median
lines(xpred,ci[3,],col=3,lty=2)	## upper CI
lines(xpred,pi[1,],col=4,lty=2)	## lower PI
lines(xpred,pi[2,],col=4,lty=2)	## upper PI

```


## Logistic model

```{r, echo=TRUE, warning=FALSE}

fire_year_whalen<-na.omit(final$time_since_last_burn[final$collection_method== "Whalen" |final$collection_method == "Drag" ])
fire_year_trap<-na.omit(final$time_since_last_burn[final$collection_method== "trap" | final$collection_method== "people" ])
fire_year_whalen<-fire_year_whalen[-c(9,10)]

n_whalen<-length(x_whalen)
n_trap<-length(x_trap)

data <- list(x_whalen = x_whalen, x_trap= x_trap, n_trap = c(n_trap, length(veg_trap)), n_whalen = c(n_whalen, length(veg_whalen)), veg_whalen = veg_whalen, veg_trap = veg_trap, fire_year_trap= fire_year_trap, fire_year_whalen= fire_year_whalen)


logistic = "
model {
r ~  dunif(min, max)	## prior population growth
N_0 ~ dunif(prob, size) ## prior on surviving population after fire

  for(i in 1:n_whalen[1]){
	   mu[i]<-N_0*exp(r*fire_year_whalen[i])
    x_whalen[i] ~ dpois((mu[i]*efficiency*time_whalen)*area_whalen)
  }
for(i in 1:n_trap[1]){
    mu[n_whalen[1]+i]<-N_0*exp(r*fire_year_trap[i])
    x_trap[i] ~ dpois(mu[i]*area_trap*time_trap)
  }

}"


## Specify priors
data$min <- 0.0001      ## uniform prior
data$max <-10  ## uniform prior 
data$size <-200  
data$prob<- 0.001 ## very low

## Specify "known" values 
data$efficiency<-0.0001  # otherwise not seperable
data$area_whalen <- 1/500 # meters squared
data$area_trap <- 1/5 # meters squared
data$time_whalen<-3.5 # This is an average of a varaible unit (2-5 hours /plot). Could model this, but would then wouldn't be seperatble from efficietcy or mu
data$time_trap<-24 # hours


## initial conditions
nchain = 3
inits <- list()
for(i in 1:nchain){
 inits[[i]] <- list(r = c(3))
}

j.model.logistic   <- rjags::jags.model(file = textConnection(logistic),
                             data = data,
                             inits = inits,
                             n.chains = nchain)

var.out.logistic   <-rjags::coda.samples(model = j.model.logistic, variable.names = c("N_0", "r"), n.iter = 4000)

```

```{r, include = FALSE, echo=TRUE}

#coda::gelman.plot(var.out.logistic)

var.burn.logistic<-window(var.out.logistic, 2100)
var.mat.logistic<-as.matrix(var.burn.logistic)

print(coda::effectiveSize(var.burn.logistic)) #pleanty 
print(summary(var.burn.logistic))
```

** Results **
```{r, echo= TRUE, warning=FALSE}


## credible and prediction intervals
nsamp <- 300
samp <- sample.int(nrow(var.mat.logistic),nsamp)
xpred <- 0:200  					## sequence of x values we're going to
npred <- length(xpred)				##      make predictions for
ypred <- matrix(0.0,nrow=nsamp,ncol=npred)	## storage for predictive interval
ycred <- matrix(0.0,nrow=nsamp,ncol=npred)	## storage for credible interval

for(g in seq_len(nsamp)){
  r = var.mat.logistic[samp[g],"r"]
  N_0 = var.mat.logistic[samp[g],"N_0"]
  ycred[g,] <- xpred*N_0[g]*exp(r[g]*c(fire_year_trap, fire_year_whalen)[g])
  ypred[g,] <- rnorm(npred,ycred[g,]*data$area_trap*data$time_trap,ycred[g,]*data$area_trap*data$time_trap)
}

ci <- apply(ycred,2,quantile,c(0.025,0.5,0.975), na.rm = TRUE)  ## credible interval and median
pi <- apply(ypred,2,quantile,c(0.025,0.975), na.rm = TRUE)		## prediction interval

plot(final$time_since_last_burn, final$count, xlab= "Year Since Last Fire", ylab = "Tick Counts", ylim = c(0, 150))
lines(xpred,ci[1,],col=3,lty=2)	## lower CI
lines(xpred,ci[2,],col=3,lwd=3)	## median
lines(xpred,ci[3,],col=3,lty=2)	## upper CI
lines(xpred,pi[1,],col=4,lty=2)	## lower PI
lines(xpred,pi[2,],col=4,lty=2)	## upper PI

```


## Logistic model with a carying capacity

```{r, echo=TRUE, warning=FALSE}

fire_year_whalen<-na.omit(final$time_since_last_burn[final$collection_method== "Whalen" |final$collection_method == "Drag" ])
fire_year_trap<-na.omit(final$time_since_last_burn[final$collection_method== "trap" | final$collection_method== "people" ])
fire_year_whalen<-fire_year_whalen[-c(9,10)]

n_whalen<-length(x_whalen)
n_trap<-length(x_trap)

data <- list(x_whalen = x_whalen, x_trap= x_trap, n_trap = c(n_trap, length(veg_trap)), n_whalen = c(n_whalen, length(veg_whalen)), veg_whalen = veg_whalen, veg_trap = veg_trap, fire_year_trap= fire_year_trap, fire_year_whalen= fire_year_whalen)


logistic.full = "
model {
r ~  dunif(min1, max1)	## prior population growth
N_0 ~ dunif(prob, size) ## prior on surviving population after fire
phi ~ dunif(min,max)  	## prior on metabolic rate

  for(i in 1:n_whalen[1]){
     K[i] <- veg_whalen[i]*phi
     A[i] <- (K[i] - N_0) / N_0
	   mu[i] <- (K[i] / (1 + A[i] * exp( -r * fire_year_whalen[i]))) ##  bring together process model

    x_whalen[i] ~ dpois((mu[i]*efficiency*time_whalen)*area_whalen) # data model
  }
for(i in 1:n_trap[1]){
    K[i+n_whalen[1]] <- veg_trap[i]*phi
     A[i+n_whalen[1]] <- (K[i+n_whalen[1]] - N_0) / N_0
    mu[n_whalen[1]+i]<- (K[i] / (1 + A[i] * exp( -r * fire_year_trap[i]))) ##  bring together process model

    x_trap[i] ~ dpois(mu[i]*area_trap*time_trap)  # data model
  }

}"


## Specify priors
data$min1 <- 0.0001      ## uniform prior
data$max1 <-10  ## uniform prior 
data$size <-200   
data$prob<- 0.001 ## very low
data$min <- 0.001      ##  uniform prior
data$max <-10  ## uniform prior

## Specify "known" values 
data$efficiency<-0.0001  # otherwise not seperable
data$area_whalen <- 1/500 # meters squared
data$area_trap <- 1/5 # meters squared
data$time_whalen<-3.5 # This is an average of a varaible unit (2-5 hours /plot). Could model this, but would then wouldn't be seperatble from efficietcy or mu
data$time_trap<-24 # hours


## initial conditions
nchain = 3
inits <- list()
for(i in 1:nchain){
 inits[[i]] <- list(r = c(3))
}

j.model.logistic.full   <- rjags::jags.model(file = textConnection(logistic.full),
                             data = data,
                             inits = inits,
                             n.chains = nchain)


var.out.logistic.full   <-rjags::coda.samples(model = j.model.logistic.full, variable.names = c("N_0", "r", "phi"), n.iter = 6000)

```

```{r, include = FALSE, echo=TRUE}

#coda::gelman.plot(var.out.logistic.full)

var.burn.logistic.full<-window(var.out.logistic.full, 5000)
var.mat.logistic.full<-as.matrix(var.burn.logistic.full)

print(coda::effectiveSize(var.burn.logistic.full)) #pleanty 
print(summary(var.burn.logistic.full))
```

** Results **

```{r, echo= TRUE, warning=FALSE}

## credible and prediction intervals
nsamp <- 300
samp <- sample.int(nrow(var.mat.logistic.full),nsamp)
xpred <- 1:200  					## sequence of x values we're going to
npred <- length(xpred)				##      make predictions for
ypred <- matrix(0.0,nrow=nsamp,ncol=npred)	## storage for predictive interval
ycred <- matrix(0.0,nrow=nsamp,ncol=npred)	## storage for credible interval
A<-rep(NA, nsamp) # temporary variables
K<-rep(NA, nsamp)  # temporary variables

for(g in seq_len(nsamp)){
  phi = var.mat.logistic.full[samp[g],"phi"]
  r = var.mat.logistic.full[samp[g],"r"]
  N_0 = var.mat.logistic.full[samp[g],"N_0"]
  
  K[g]<- K[i] <- c(veg_whalen, veg_trap)[g]*phi
  A[g]<- (K[g] - N_0) / N_0
    
  ycred[g,] <- K[g]/(xpred*A[g]*exp(-r[g]*c(fire_year_trap, fire_year_whalen)[g]))
  ypred[g,] <- rnorm(npred,ycred[g,]*data$area_trap*data$time_trap,ycred[g,]*data$area_trap*data$time_trap)
}

ci <- apply(ycred,2,quantile,c(0.025,0.5,0.975), na.rm = TRUE)  ## credible interval and median
pi <- apply(ypred,2,quantile,c(0.025,0.975), na.rm = TRUE)		## prediction interval

plot(final$time_since_last_burn, final$count, xlab= "Year Since Last Fire", ylab = "Tick Counts", ylim = c(0, 100))
lines(xpred,ci[1,],col=3,lty=2)	## lower CI
lines(xpred,ci[2,],col=3,lwd=3)	## median
lines(xpred,ci[3,],col=3,lty=2)	## upper CI
lines(xpred,pi[1,],col=4,lty=2)	## lower PI
lines(xpred,pi[2,],col=4,lty=2)	## upper PI

plot(final$litter_mass_wet, final$count, xlab= "Litter Biomass", ylab = "Tick Counts", ylim = c(0, 100))
lines(xpred,ci[1,],col=3,lty=2)	## lower CI
lines(xpred,ci[2,],col=3,lwd=3)	## median
lines(xpred,ci[3,],col=3,lty=2)	## upper CI
lines(xpred,pi[1,],col=4,lty=2)	## lower PI
lines(xpred,pi[2,],col=4,lty=2)	## upper PI

```

## Model Comparison 

```{r, echo= TRUE}
DIC<-list()
DIC$mean<-dic.samples(j.model.mean, 2000)
DIC$regression<-dic.samples(j.model.regression, 2000)
DIC$logistic<-dic.samples(j.model.logistic, 2000)
DIC$logistic.full<-dic.samples(j.model.logistic.full, 2000)

print(paste("Mean DIC:"))
DIC$mean
print(paste("Regression DIC:"))
DIC$regression
print(paste("Logistic DIC:"))
DIC$logistic
print(paste("Logistic With Carying Capacity DIC:"))
 DIC$logistic.full
```

## Discussion 

These analysis was meant to compare different population models. Using DIC as a metric, I was going to compare my mean-only model to my regression model, and my regression model to my two logistic models. From there, I would hypothesize about how the different components of each model (litter biomass, year since burnt) might be affecting tick populations. It could be that the variables themselves aren't what made my particular mode a good fit, but the functional form of the model. I could then explore this line of reasoning by seeing if I took the model with the lower DIC and switched the variables involved if I got a better DIC. 

However, I think my analysis revealed that my models aren't at a place where they are representative of structure. As it stands the "fixed values" I use have a disproportionately large affect on the analysis. I think this is especially clear in the mean model analysis. Despite being loaded with a large prior, the final mean resembles the Whalen sampling method more closely than the trap sampling method. I know that the trap sampling method constitutes a large fraction of the data, so I suspect this result is due to the choice to put tick counts into density by dividing by the area and the efficiency. Both area and efficient are tied to one another- and hard to quantify. I don't think that Whalen evenly covered every centimeter of the 500 m^2 plots, and efficiency is a number I chose at random. Improving this analysis means getting a more representative (and separable) estimate of efficiency.  

Looking at the analysis I do have, it seems tentatively like a logistic-based population model will have the lowest DIC. My confidence and predictive intervals speak to some errors in CI and PI making  - especially in my predictive intervals. 
